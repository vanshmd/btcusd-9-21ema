"""
EMA Pullback Strategy + 50k+ Filter Optimizer (No Lookahead, Low-Overfit)
========================================================================

You asked for:
- MUCH larger search (>50,000 unique configs)
- More indicator-based filters (e.g., ROC) + more regimes
- Avoid lookahead bias
- Avoid overfitting (IMPORTANT): do NOT stack many indicators at once

This script enforces **LOW-COMPLEXITY configs**:
------------------------------------------------
Each tested configuration is constrained to:
  - at most ONE calendar filter (time-of-day OR day-of-week OR none)
  - at most ONE indicator filter group (RSI OR ROC OR ADX OR ATR-regime OR Volume OR Bollinger OR Candle-size OR EMA200-regime OR none)
  - optional side filter (both / long-only / short-only)

So you can test 50k+ configs without exploring high-dimensional combinations.

Ranking goal (requested):
-------------------------
Find strategies that have **strong compounded returns on BOTH Train and OOS**.
We rank by a robust score = min(train_CAGR, oos_CAGR) with trade-count constraints.

Core guarantees
---------------
- Signals use current candle only (no future candles).
- Entry executes on NEXT candle via stop-entry at signal candle high/low.
- Stops/targets based on signal candle.
- If stop & target hit in same bar, stop assumed first (conservative).
- Fees + slippage applied against you.

Install
-------
pip install numpy pandas requests numba pyarrow

Run
---
python btc_ema_filter_optimizer_50k_low_overfit.py

Outputs
-------
- btc_filter_search_summary.csv : all tested configs + full/train/oos metrics
- btc_top20_filters.csv         : top 20 configs ranked by robust_cagr_pct (min(train_cagr, oos_cagr))
- btc_best_filter_trades.csv    : detailed trades for the best config by robust score

DISCLAIMER
----------
Research/backtesting only. Not financial advice.
"""

from __future__ import annotations

import os
import math
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import requests

try:
    from numba import njit  # type: ignore
except Exception:  # pragma: no cover
    njit = None


# ==============================
# CONFIG - EDIT THESE PARAMETERS
# ==============================

# Backtest date range in *local* timezone (IST = UTC+5:30)
START_DATE = "2020-01-01"
END_DATE   = "2025-12-31"

# Train / OOS split (OOS starts the next day after TRAIN_END_DATE)
TRAIN_END_DATE = "2024-12-31"

# Trading instrument (Delta Exchange spot/indices use different symbols; you used BTCUSD)
SYMBOL = "BTCUSD"

# Timeframes to test (Delta resolution strings)
TIMEFRAMES = ["5m"]

# Local timezone (India)
LOCAL_TZ = timezone(timedelta(hours=5, minutes=30))

# Capital & risk
INITIAL_CAPITAL         = 30000.0
RISK_PER_TRADE          = 0.01
TAKE_PROFIT_R_MULTIPLE  = 3.0

# EMA settings
EMA_SHORT        = 9
EMA_LONG         = 15
EMA_TREND_FILTER = 200
EMA_9_200_MIN_DIST = 0.003  # base strategy requires EMA9 vs EMA200 distance >= 0.3%

# Trend / slope / angle settings
EMA_SLOPE_LOOKBACK      = 3
MIN_SLOPE_PCT           = 0.0001
EMA_LONG_MIN_ANGLE_DEG  = 33.0
EMA_LONG_ANGLE_SCALE    = 500.0
MIN_EMA_SEPARATION_PCT  = 0.0003
CROSS_LOOKBACK          = 10

# Candle pattern parameters
PINBAR_WICK_RATIO = 1.2
PINBAR_MAX_WICK_BODY_RATIO = 1.2
STRONG_BODY_MIN_BODY_TO_RANGE = 0.4

# Trading costs
BROKERAGE_TOTAL_RATE    = 0.0006
SLIPPAGE_TOTAL_RATE     = 0.0002
BROKERAGE_HALF_RATE     = BROKERAGE_TOTAL_RATE / 2.0
SLIPPAGE_HALF_RATE      = SLIPPAGE_TOTAL_RATE / 2.0

# Trade-count constraints (IMPORTANT to avoid tiny-sample "best" configs)
MIN_TRADES_FULL_FOR_RANK   = 30
MIN_TRADES_TRAIN_FOR_RANK  = 120   # Train is multi-year: require decent sample size
MIN_TRADES_OOS_FOR_RANK    = 40    # OOS is 1 year: enforce enough trades

# Require profitability in BOTH segments for ranking
REQUIRE_POSITIVE_TRAIN_AND_OOS = True

# Search size
RANDOM_SEED        = 42
N_RANDOM_CONFIGS   = 50000   # target unique random configs (in addition to systematic sweeps)

# Candles API chunk size
MAX_CANDLES_PER_REQUEST = 4000

# Output paths
RESULTS_SUMMARY_CSV_PATH = "btc_filter_search_summary.csv"
BEST_TRADES_CSV_PATH     = "btc_best_filter_trades.csv"
TOP_N_FILTERS_TO_SAVE   = 20
TOP_FILTERS_CSV_PATH    = "btc_top20_filters.csv"

# Optional: cache candles to avoid re-downloading
USE_CACHE = True
CACHE_DIR = "candle_cache"


# ==============================
# DELTA API (public endpoints)
# ==============================

DELTA_BASE_URL = "https://api.delta.exchange"
USER_AGENT = "python-ema-filter-optimizer-low-overfit"


def delta_request(method: str, path: str, params: Optional[Dict] = None) -> Dict:
    url = f"{DELTA_BASE_URL}{path}"
    params = params or {}
    headers = {"User-Agent": USER_AGENT, "Content-Type": "application/json"}

    resp = requests.request(method=method, url=url, params=params, timeout=(5, 60), headers=headers)
    resp.raise_for_status()
    data = resp.json()
    if isinstance(data, dict) and data.get("success") is False:
        raise RuntimeError(f"Delta API error: {data.get('error')}")
    return data.get("result", data)


def resolution_to_seconds(resolution: str) -> int:
    res = resolution.strip().lower()
    unit = res[-1]
    value = int(res[:-1])
    if unit == "m":
        return value * 60
    if unit == "h":
        return value * 60 * 60
    if unit == "d":
        return value * 24 * 60 * 60
    if unit == "w":
        return value * 7 * 24 * 60 * 60
    raise ValueError(f"Unsupported resolution: {resolution}")


def local_date_range_to_utc_epochs(start_date: str, end_date: str, local_tz: timezone) -> Tuple[int, int]:
    local_start = datetime.strptime(start_date, "%Y-%m-%d").replace(tzinfo=local_tz)
    local_end = (
        datetime.strptime(end_date, "%Y-%m-%d").replace(tzinfo=local_tz)
        + timedelta(days=1) - timedelta(seconds=1)
    )
    start_utc = local_start.astimezone(timezone.utc)
    end_utc = local_end.astimezone(timezone.utc)
    return int(start_utc.timestamp()), int(end_utc.timestamp())


def _cache_path(symbol: str, resolution: str, start_date: str, end_date: str) -> str:
    os.makedirs(CACHE_DIR, exist_ok=True)
    fname = f"{symbol}_{resolution}_{start_date}_{end_date}.parquet"
    return os.path.join(CACHE_DIR, fname)


def fetch_ohlc_from_delta(symbol: str, resolution: str, start_date: str, end_date: str) -> pd.DataFrame:
    if USE_CACHE:
        path = _cache_path(symbol, resolution, start_date, end_date)
        if os.path.exists(path):
            df = pd.read_parquet(path)
            df.index = pd.to_datetime(df.index)
            if df.index.tz is None:
                df.index = df.index.tz_localize(LOCAL_TZ)
            return df

    start_ts, end_ts = local_date_range_to_utc_epochs(start_date, end_date, LOCAL_TZ)
    res_seconds = resolution_to_seconds(resolution)
    max_span = res_seconds * MAX_CANDLES_PER_REQUEST

    all_records: List[Dict] = []
    print(f"Fetching {symbol} {resolution} candles from {start_date} to {end_date} (LOCAL {LOCAL_TZ})")

    current_start = start_ts
    path = "/v2/history/candles"
    while current_start < end_ts:
        current_end = min(current_start + max_span - 1, end_ts)
        params = {"symbol": symbol, "resolution": resolution, "start": current_start, "end": current_end}
        result = delta_request("GET", path, params=params)
        if not result:
            break
        all_records.extend(result)
        current_start = current_end + 1

    if not all_records:
        raise RuntimeError("No candle data returned from Delta Exchange for given range.")

    df = pd.DataFrame(all_records)
    if "time" not in df.columns:
        raise RuntimeError(f"Unexpected candle format. Columns: {df.columns.tolist()}")

    df["time"] = pd.to_datetime(df["time"], unit="s", utc=True).dt.tz_convert(LOCAL_TZ)
    for col in ("open", "high", "low", "close", "volume"):
        if col not in df.columns:
            raise RuntimeError(f"Expected column '{col}' not found in candles data.")
        df[col] = df[col].astype(float)

    df = df.sort_values("time").set_index("time")[["open", "high", "low", "close", "volume"]].copy()

    if USE_CACHE:
        df.to_parquet(_cache_path(symbol, resolution, start_date, end_date))

    print(f"Fetched {len(df)} candles. Range: {df.index[0]} -> {df.index[-1]} (LOCAL)")
    return df


# ==============================
# INDICATORS (no lookahead)
# ==============================

def compute_rsi(close: pd.Series, window: int = 14) -> pd.Series:
    delta = close.diff()
    gain = delta.clip(lower=0.0)
    loss = (-delta).clip(lower=0.0)
    avg_gain = gain.ewm(alpha=1.0 / window, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1.0 / window, adjust=False).mean()
    rs = avg_gain / avg_loss.replace(0.0, np.nan)
    return 100.0 - (100.0 / (1.0 + rs))


def compute_atr(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:
    prev_close = close.shift(1)
    tr = pd.concat([(high - low), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)
    return tr.ewm(alpha=1.0 / window, adjust=False).mean()


def compute_adx(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:
    up_move = high.diff()
    down_move = -low.diff()

    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)
    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)

    prev_close = close.shift(1)
    tr = pd.concat([(high - low), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)

    atr = tr.ewm(alpha=1.0 / window, adjust=False).mean()
    plus_dm_sm = pd.Series(plus_dm, index=high.index).ewm(alpha=1.0 / window, adjust=False).mean()
    minus_dm_sm = pd.Series(minus_dm, index=high.index).ewm(alpha=1.0 / window, adjust=False).mean()

    plus_di = 100.0 * (plus_dm_sm / atr.replace(0.0, np.nan))
    minus_di = 100.0 * (minus_dm_sm / atr.replace(0.0, np.nan))

    dx = 100.0 * (plus_di - minus_di).abs() / (plus_di + minus_di).replace(0.0, np.nan)
    return dx.ewm(alpha=1.0 / window, adjust=False).mean()


def compute_bollinger(close: pd.Series, window: int = 20, num_std: float = 2.0) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:
    """Returns (mid, upper, lower, zscore)"""
    mid = close.rolling(window).mean()
    std = close.rolling(window).std(ddof=0)
    upper = mid + num_std * std
    lower = mid - num_std * std
    z = (close - mid) / std.replace(0.0, np.nan)
    return mid, upper, lower, z


def add_indicators(df: pd.DataFrame, resolution: str) -> pd.DataFrame:
    """
    Add all indicators used for filters.

    Note: all rolling-mean "regime baselines" are shifted(1) where appropriate,
    so comparisons like "volume > k * avg_volume" use a past-only average.
    """
    df = df.copy()
    close = df["close"]

    # EMAs
    df["ema_short"] = close.ewm(span=EMA_SHORT, adjust=False).mean()
    df["ema_long"]  = close.ewm(span=EMA_LONG,  adjust=False).mean()
    df["ema_200"]   = close.ewm(span=EMA_TREND_FILTER, adjust=False).mean()

    # EMA slopes/angles (for base trend rules)
    ema_short_prev = df["ema_short"].shift(EMA_SLOPE_LOOKBACK)
    ema_long_prev  = df["ema_long"].shift(EMA_SLOPE_LOOKBACK)

    slope_short_pct = (df["ema_short"] - ema_short_prev) / ema_short_prev
    slope_long_pct  = (df["ema_long"]  - ema_long_prev)  / ema_long_prev

    df["slope_short_pct"] = slope_short_pct
    df["slope_long_pct"]  = slope_long_pct

    df["ema_short_angle_deg"] = np.degrees(np.arctan(slope_short_pct * EMA_LONG_ANGLE_SCALE))
    df["ema_long_angle_deg"]  = np.degrees(np.arctan(slope_long_pct  * EMA_LONG_ANGLE_SCALE))

    df["ema_sep_pct"] = (df["ema_short"] - df["ema_long"]).abs() / df["close"].replace(0.0, np.nan)

    # recent EMA cross filter
    df["ema_diff_sign"] = np.sign(df["ema_short"] - df["ema_long"])
    df["ema_cross"] = df["ema_diff_sign"].ne(df["ema_diff_sign"].shift(1))
    df["recent_cross"] = df["ema_cross"].rolling(CROSS_LOOKBACK).max().fillna(0).astype(bool)

    df["long_trend"] = (
        (df["ema_short"] > df["ema_long"]) &
        (df["slope_long_pct"] > MIN_SLOPE_PCT) &
        (df["ema_long_angle_deg"] >= EMA_LONG_MIN_ANGLE_DEG) &
        (df["ema_sep_pct"] > MIN_EMA_SEPARATION_PCT) &
        (~df["recent_cross"])
    )

    df["short_trend"] = (
        (df["ema_short"] < df["ema_long"]) &
        (df["slope_long_pct"] < -MIN_SLOPE_PCT) &
        (df["ema_long_angle_deg"] <= -EMA_LONG_MIN_ANGLE_DEG) &
        (df["ema_sep_pct"] > MIN_EMA_SEPARATION_PCT) &
        (~df["recent_cross"])
    )

    # RSI
    df["rsi_14"] = compute_rsi(close, 14)
    df["rsi_slope"] = df["rsi_14"] - df["rsi_14"].shift(1)

    # ATR / ADX
    df["atr_14"] = compute_atr(df["high"], df["low"], close, 14)
    df["atr_pct_14"] = df["atr_14"] / close.replace(0.0, np.nan)
    df["adx_14"] = compute_adx(df["high"], df["low"], close, 14)

    # Bollinger
    bb_mid, bb_up, bb_lo, bb_z = compute_bollinger(close, 20, 2.0)
    df["bb_mid_20"] = bb_mid
    df["bb_up_20"] = bb_up
    df["bb_lo_20"] = bb_lo
    df["bb_z_20"] = bb_z
    df["bb_width_pct_20"] = (bb_up - bb_lo) / close.replace(0.0, np.nan)

    # ROC (coarse horizons based on timeframe)
    res_seconds = resolution_to_seconds(resolution)
    bars_1h = max(1, int(round(3600 / res_seconds)))
    bars_2h = max(1, int(round(2 * 3600 / res_seconds)))
    bars_4h = max(1, int(round(4 * 3600 / res_seconds)))

    for b, name in [(bars_1h, "roc_1h"), (bars_2h, "roc_2h"), (bars_4h, "roc_4h")]:
        df[name] = close.pct_change(periods=b)

    # Candle size features
    rng = (df["high"] - df["low"])
    body = (df["close"] - df["open"]).abs()
    df["range_atr"] = rng / df["atr_14"].replace(0.0, np.nan)
    df["body_atr"]  = body / df["atr_14"].replace(0.0, np.nan)

    # EMA9-EMA200 distance (already in base rules; useful for extra regime filter)
    df["dist_9_200"] = (df["ema_short"] - df["ema_200"]).abs() / close.replace(0.0, np.nan)

    # EMA200 slope regimes (longer horizon, coarse)
    ema200_prev = df["ema_200"].shift(bars_4h)
    df["ema200_slope_4h"] = (df["ema_200"] - ema200_prev) / ema200_prev

    # Timestamp features (LOCAL timezone already)
    idx = df.index
    df["minute_of_day"] = idx.hour * 60 + idx.minute
    df["day_of_week"] = idx.dayofweek  # 0=Mon ... 6=Sun

    # Rolling avg volume (shifted => past-only baseline)
    for lb in (20, 50, 100, 200):
        df[f"vol_sma_{lb}"] = df["volume"].rolling(lb).mean().shift(1)

    # Rolling ATR% mean for vol regime (shifted => past-only baseline)
    for lb in (50, 100, 200):
        df[f"atrpct_mean_{lb}"] = df["atr_pct_14"].rolling(lb).mean().shift(1)

    return df


# ==============================
# BASE STRATEGY SIGNALS (vectorized, no lookahead)
# ==============================

def build_base_signals(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    o = df["open"]
    h = df["high"]
    l = df["low"]
    c = df["close"]

    body = (c - o).abs()
    rng = (h - l)
    upper_wick = h - np.maximum(o, c)
    lower_wick = np.minimum(o, c) - l

    is_bull = c > o
    is_bear = c < o

    bullish_pinbar = (
        is_bull &
        (body > 0) &
        (lower_wick >= PINBAR_WICK_RATIO * body) &
        (upper_wick <= PINBAR_MAX_WICK_BODY_RATIO * body)
    )
    bearish_pinbar = (
        is_bear &
        (body > 0) &
        (upper_wick >= PINBAR_WICK_RATIO * body) &
        (lower_wick <= PINBAR_MAX_WICK_BODY_RATIO * body)
    )

    strong_bull = is_bull & (rng > 0) & ((body / rng) >= STRONG_BODY_MIN_BODY_TO_RANGE)
    strong_bear = is_bear & (rng > 0) & ((body / rng) >= STRONG_BODY_MIN_BODY_TO_RANGE)

    ema_s = df["ema_short"]
    ema_l = df["ema_long"]
    ema200 = df["ema_200"]

    touches = ((l <= ema_s) & (ema_s <= h)) | ((l <= ema_l) & (ema_l <= h))

    dist_9_200 = df["dist_9_200"]

    ok_ind = ~(ema_s.isna() | ema_l.isna() | ema200.isna() | c.isna())

    long_signal_raw = (
        ok_ind &
        df["long_trend"] &
        touches &
        (ema_s > ema200) &
        (dist_9_200 >= EMA_9_200_MIN_DIST) &
        (c > ema_s) & (c > ema_l) &
        (bullish_pinbar | strong_bull | is_bull)
    )

    short_signal_raw = (
        ok_ind &
        df["short_trend"] &
        touches &
        (ema200 > ema_s) &
        (dist_9_200 >= EMA_9_200_MIN_DIST) &
        (c < ema_s) & (c < ema_l) &
        (bearish_pinbar | strong_bear | is_bear)
    )

    df["long_signal_raw"] = long_signal_raw.fillna(False)
    df["short_signal_raw"] = short_signal_raw.fillna(False)

    print(f"Base signals: long={int(df['long_signal_raw'].sum())}, short={int(df['short_signal_raw'].sum())}")
    return df


# ==============================
# FAST BACKTEST (Numba)
# ==============================

if njit is not None:

    @njit
    def _simulate_one(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        long_signal: np.ndarray,
        short_signal: np.ndarray,
        initial_capital: float,
        risk_per_trade: float,
        tp_r_multiple: float,
        brokerage_half_rate: float,
        slippage_half_rate: float,
    ) -> Tuple[float, float, float, float, float, float, float, float, float]:
        """
        Returns:
          total_trades, win_rate, simple_return_pct, compounded_return_pct, max_drawdown_pct,
          sharpe, profit_factor, avg_return_pct, final_equity
        """
        n = close.shape[0]
        equity = initial_capital
        peak_equity = equity
        max_dd = 0.0  # negative

        total_trades = 0.0
        wins = 0.0
        losses = 0.0
        sum_ret = 0.0
        compounded = 1.0

        sum_pos_ret = 0.0
        sum_neg_ret = 0.0

        # sharpe via Welford
        count = 0.0
        mean = 0.0
        m2 = 0.0

        in_trade = False
        side = 0  # 1 long, -1 short

        entry_price = 0.0
        stop_loss = 0.0
        target = 0.0
        size = 0.0
        entry_equity = 0.0

        for i in range(1, n):
            bar_high = high[i]
            bar_low = low[i]

            if in_trade:
                exit_price = 0.0
                hit = 0  # 0 none, 1 stop, 2 target, 3 both=>stop first

                if side == 1:
                    stop_hit = bar_low <= stop_loss
                    target_hit = bar_high >= target
                    if stop_hit and target_hit:
                        exit_price = stop_loss
                        hit = 3
                    elif stop_hit:
                        exit_price = stop_loss
                        hit = 1
                    elif target_hit:
                        exit_price = target
                        hit = 2
                else:
                    stop_hit = bar_high >= stop_loss
                    target_hit = bar_low <= target
                    if stop_hit and target_hit:
                        exit_price = stop_loss
                        hit = 3
                    elif stop_hit:
                        exit_price = stop_loss
                        hit = 1
                    elif target_hit:
                        exit_price = target
                        hit = 2

                if hit != 0:
                    if side == 1:
                        entry_eff = entry_price * (1.0 + slippage_half_rate)
                        exit_eff = exit_price * (1.0 - slippage_half_rate)
                        gross_pnl = (exit_eff - entry_eff) * size
                    else:
                        entry_eff = entry_price * (1.0 - slippage_half_rate)
                        exit_eff = exit_price * (1.0 + slippage_half_rate)
                        gross_pnl = (entry_eff - exit_eff) * size

                    cost_entry = abs(entry_eff * size) * brokerage_half_rate
                    cost_exit = abs(exit_eff * size) * brokerage_half_rate
                    net_pnl = gross_pnl - (cost_entry + cost_exit)

                    trade_ret = 0.0
                    if entry_equity != 0.0:
                        trade_ret = net_pnl / entry_equity

                    equity = equity * (1.0 + trade_ret)

                    total_trades += 1.0
                    if net_pnl > 0.0:
                        wins += 1.0
                    elif net_pnl < 0.0:
                        losses += 1.0

                    sum_ret += trade_ret
                    compounded = compounded * (1.0 + trade_ret)
                    if trade_ret > 0.0:
                        sum_pos_ret += trade_ret
                    elif trade_ret < 0.0:
                        sum_neg_ret += trade_ret

                    # Welford update
                    count += 1.0
                    delta = trade_ret - mean
                    mean = mean + delta / count
                    delta2 = trade_ret - mean
                    m2 = m2 + delta * delta2

                    if equity > peak_equity:
                        peak_equity = equity
                    dd = (equity - peak_equity) / peak_equity
                    if dd < max_dd:
                        max_dd = dd

                    in_trade = False
                    continue

            if not in_trade:
                # LONG entry (prev candle signal)
                if long_signal[i - 1]:
                    ep = high[i - 1]
                    sl = low[i - 1]
                    risk_per_unit = ep - sl
                    if risk_per_unit > 0.0 and bar_high >= ep:
                        entry_price = ep
                        stop_loss = sl
                        entry_equity = equity
                        risk_amount = entry_equity * risk_per_trade
                        size = risk_amount / risk_per_unit
                        target = entry_price + tp_r_multiple * risk_per_unit
                        side = 1
                        in_trade = True
                        continue

                # SHORT entry (prev candle signal)
                if short_signal[i - 1]:
                    ep = low[i - 1]
                    sl = high[i - 1]
                    risk_per_unit = sl - ep
                    if risk_per_unit > 0.0 and bar_low <= ep:
                        entry_price = ep
                        stop_loss = sl
                        entry_equity = equity
                        risk_amount = entry_equity * risk_per_trade
                        size = risk_amount / risk_per_unit
                        target = entry_price - tp_r_multiple * risk_per_unit
                        side = -1
                        in_trade = True
                        continue

        # EOD close
        if in_trade:
            exit_price = close[n - 1]
            if side == 1:
                entry_eff = entry_price * (1.0 + slippage_half_rate)
                exit_eff = exit_price * (1.0 - slippage_half_rate)
                gross_pnl = (exit_eff - entry_eff) * size
            else:
                entry_eff = entry_price * (1.0 - slippage_half_rate)
                exit_eff = exit_price * (1.0 + slippage_half_rate)
                gross_pnl = (entry_eff - exit_eff) * size

            cost_entry = abs(entry_eff * size) * brokerage_half_rate
            cost_exit = abs(exit_eff * size) * brokerage_half_rate
            net_pnl = gross_pnl - (cost_entry + cost_exit)

            trade_ret = 0.0
            if entry_equity != 0.0:
                trade_ret = net_pnl / entry_equity

            equity = equity * (1.0 + trade_ret)

            total_trades += 1.0
            if net_pnl > 0.0:
                wins += 1.0
            elif net_pnl < 0.0:
                losses += 1.0

            sum_ret += trade_ret
            compounded = compounded * (1.0 + trade_ret)
            if trade_ret > 0.0:
                sum_pos_ret += trade_ret
            elif trade_ret < 0.0:
                sum_neg_ret += trade_ret

            count += 1.0
            delta = trade_ret - mean
            mean = mean + delta / count
            delta2 = trade_ret - mean
            m2 = m2 + delta * delta2

            if equity > peak_equity:
                peak_equity = equity
            dd = (equity - peak_equity) / peak_equity
            if dd < max_dd:
                max_dd = dd

        # metrics
        win_rate = 0.0
        if total_trades > 0.0:
            win_rate = (wins / total_trades) * 100.0

        simple_return_pct = sum_ret * 100.0
        compounded_return_pct = (compounded - 1.0) * 100.0
        max_drawdown_pct = (-max_dd) * 100.0

        sharpe = 0.0
        if count > 1.0:
            var = m2 / (count - 1.0)
            if var > 0.0:
                std = math.sqrt(var)
                sharpe = (mean / std) * math.sqrt(252.0)

        profit_factor = 0.0
        if sum_neg_ret < 0.0:
            profit_factor = sum_pos_ret / (-sum_neg_ret)

        avg_return_pct = 0.0
        if total_trades > 0.0:
            avg_return_pct = (sum_ret / total_trades) * 100.0

        return (
            total_trades,
            win_rate,
            simple_return_pct,
            compounded_return_pct,
            max_drawdown_pct,
            sharpe,
            profit_factor,
            avg_return_pct,
            equity,
        )

else:
    def _simulate_one(*args, **kwargs):
        raise RuntimeError("Numba is required. Install: pip install numba")


# ==============================
# FILTER CONFIG (LOW OVERFIT)
# ==============================

@dataclass(frozen=True)
class FilterConfig:
    calendar_filter: str     # "cal:none" OR a time-of-day skip filter OR a DOW filter
    side_filter: str         # side:both / side:long_only / side:short_only
    indicator_group: str     # ind:none / ind:vol / ind:rsi / ind:roc / ind:atr / ind:adx / ind:bb / ind:candle / ind:ema200
    indicator_filter: str    # a key within the selected indicator group (or "ind:none")

    def id(self) -> str:
        return "|".join([self.calendar_filter, self.side_filter, self.indicator_group, self.indicator_filter])


# ==============================
# FILTER MAP BUILDERS
# ==============================

def _allow_all(n: int) -> Tuple[np.ndarray, np.ndarray]:
    a = np.ones(n, dtype=np.bool_)
    return a, a


def make_time_filter_masks(minute_of_day: pd.Series) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """
    Time-of-day SKIP windows. True = allow signals.

    We include 1h/2h/3h/4h/5h/6h skip windows, starts every hour.
    For each window we test:
      - apply to both sides
      - apply only to long
      - apply only to short
    """
    minutes = minute_of_day.to_numpy()
    n = minutes.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)

    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"cal:none": (allow_all, allow_all)}

    lengths = [60, 120, 180, 240, 300, 360]  # minutes
    starts = list(range(0, 24 * 60, 60))     # every hour

    for length in lengths:
        for start in starts:
            end = (start + length) % (24 * 60)
            if end > start:
                in_window = (minutes >= start) & (minutes < end)
            else:
                in_window = (minutes >= start) | (minutes < end)  # wraps midnight

            base_mask = ~in_window
            hh1, mm1 = divmod(start, 60)
            hh2, mm2 = divmod(end, 60)
            tag = f"cal:skip_{hh1:02d}{mm1:02d}_{hh2:02d}{mm2:02d}"

            out[f"{tag}:both"] = (base_mask, base_mask)
            out[f"{tag}:long"] = (base_mask, allow_all)
            out[f"{tag}:short"] = (allow_all, base_mask)

    return out


def make_dow_filter_masks(day_of_week: pd.Series) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """Day-of-week filters. True = allow signals."""
    dow = day_of_week.to_numpy()
    n = dow.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}

    # coarser DOW filters (avoid too many)
    out["cal:dow_mon_fri"] = ((dow <= 4), (dow <= 4))
    out["cal:dow_weekends_only"] = ((dow >= 5), (dow >= 5))

    # skip single day (still coarse, 7 variants)
    names = ["mon", "tue", "wed", "thu", "fri", "sat", "sun"]
    for d in range(7):
        mask = dow != d
        out[f"cal:dow_skip_{names[d]}"] = (mask, mask)

    # NOTE: baseline "cal:none" is already in time map
    return out


def build_calendar_map(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """One combined calendar map (time-of-day + DOW). Only ONE can be selected at a time."""
    cal = make_time_filter_masks(df["minute_of_day"])
    dow = make_dow_filter_masks(df["day_of_week"])
    # merge: if key collisions (shouldn't), DOW overrides.
    cal.update(dow)
    return cal


def make_volume_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    multipliers = [2.0, 3.0, 5.0, 8.0]
    lookbacks = [20, 50, 100, 200]
    vol = df["volume"].to_numpy()

    for lb in lookbacks:
        avg = df[f"vol_sma_{lb}"].to_numpy()
        for m in multipliers:
            mask = (avg > 0) & (vol > (m * avg))
            out[f"vol:gt_{m}x_sma{lb}"] = (mask, mask)

    # Avoid extreme volume
    for lb in (50, 100):
        avg = df[f"vol_sma_{lb}"].to_numpy()
        mask = (avg > 0) & (vol < (2.0 * avg))
        out[f"vol:lt_2x_sma{lb}"] = (mask, mask)

    return out


def make_rsi_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    rsi = df["rsi_14"].to_numpy()
    rsi_slope = df["rsi_slope"].to_numpy()

    # coarse slope regimes
    out["rsi:slope_pos_long_slope_neg_short"] = (rsi_slope > 0, rsi_slope < 0)
    out["rsi:slope_pos_both"] = (rsi_slope > 0, rsi_slope > 0)
    out["rsi:slope_neg_both"] = (rsi_slope < 0, rsi_slope < 0)

    # coarse RSI thresholds (avoid near-duplicates like 49 vs 50)
    for thr in (40.0, 45.0, 50.0, 55.0, 60.0):
        out[f"rsi:long_gt_{thr}_short_lt_{100-thr}"] = (rsi > thr, rsi < (100.0 - thr))

    out["rsi:long_gt_50_short_lt_50"] = (rsi > 50.0, rsi < 50.0)
    out["rsi:avoid_extremes_30_70"] = ((rsi >= 30.0) & (rsi <= 70.0), (rsi >= 30.0) & (rsi <= 70.0))
    out["rsi:avoid_extremes_20_80"] = ((rsi >= 20.0) & (rsi <= 80.0), (rsi >= 20.0) & (rsi <= 80.0))

    return out


def make_atr_regime_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """ATR% regime using ratio = atr_pct / mean(atr_pct) where mean is shifted(1) => past-only baseline."""
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    atr_pct = df["atr_pct_14"].to_numpy()
    for lb in (50, 100, 200):
        mean_ = df[f"atrpct_mean_{lb}"].to_numpy()
        vol_ratio = atr_pct / np.where(mean_ == 0, np.nan, mean_)

        for thr in (1.2, 1.5, 2.0):  # coarse
            mask = np.isfinite(vol_ratio) & (vol_ratio >= thr)
            out[f"atr:high_ratio_ge_{thr}_mean{lb}"] = (mask, mask)

        for thr in (0.8, 0.7, 0.6):  # coarse
            mask = np.isfinite(vol_ratio) & (vol_ratio <= thr)
            out[f"atr:low_ratio_le_{thr}_mean{lb}"] = (mask, mask)

    return out


def make_adx_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    adx = df["adx_14"].to_numpy()

    for thr in (15.0, 20.0, 25.0, 30.0):
        out[f"adx:trending_gt_{thr}"] = (adx > thr, adx > thr)
        out[f"adx:ranging_lt_{thr}"]  = (adx < thr, adx < thr)

    # one-sided regime tests (still "single indicator group")
    out["adx:long_trend_gt20_short_ranging_lt20"] = (adx > 20.0, adx < 20.0)
    out["adx:long_ranging_lt20_short_trend_gt20"] = (adx < 20.0, adx > 20.0)

    return out


def make_roc_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """
    ROC filters using 1h/2h/4h horizons (in bars computed from timeframe).
    Coarse thresholds to avoid near-duplicates.
    """
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    for col in ("roc_1h", "roc_2h", "roc_4h"):
        roc = df[col].to_numpy()

        # direction only
        out[f"roc:{col}:pos_long_neg_short"] = (roc > 0.0, roc < 0.0)
        out[f"roc:{col}:pos_both"] = (roc > 0.0, roc > 0.0)
        out[f"roc:{col}:neg_both"] = (roc < 0.0, roc < 0.0)

        # coarse magnitude thresholds
        for thr in (0.0025, 0.005, 0.01, 0.02):  # 0.25%, 0.5%, 1%, 2%
            out[f"roc:{col}:abs_ge_{thr}"] = (np.abs(roc) >= thr, np.abs(roc) >= thr)
            out[f"roc:{col}:long_ge_{thr}_short_le_-{thr}"] = (roc >= thr, roc <= -thr)

        # low-momentum regime (sometimes helps mean-reversion vs trend-follow)
        for thr in (0.0025, 0.005):
            out[f"roc:{col}:abs_le_{thr}"] = (np.abs(roc) <= thr, np.abs(roc) <= thr)

    return out


def make_bb_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """Bollinger width / z-score regimes (coarse)."""
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    width = df["bb_width_pct_20"].to_numpy()
    z = df["bb_z_20"].to_numpy()

    # width regimes
    for thr in (0.015, 0.02, 0.03, 0.04):  # 1.5%, 2%, 3%, 4%
        out[f"bb:width_ge_{thr}"] = (width >= thr, width >= thr)
    for thr in (0.01, 0.015, 0.02):
        out[f"bb:width_le_{thr}"] = (width <= thr, width <= thr)

    # z-score regimes
    out["bb:z_pos_long_z_neg_short"] = (z > 0.0, z < 0.0)
    for thr in (0.5, 1.0, 1.5):
        out[f"bb:z_ge_{thr}"] = (z >= thr, z >= thr)
        out[f"bb:z_le_-{thr}"] = (z <= -thr, z <= -thr)

    return out


def make_candle_size_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """Filter by signal candle size relative to ATR (coarse)."""
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    range_atr = df["range_atr"].to_numpy()
    body_atr = df["body_atr"].to_numpy()

    # avoid huge signal candles
    for thr in (1.0, 1.5, 2.0):
        out[f"candle:range_atr_le_{thr}"] = (range_atr <= thr, range_atr <= thr)

    # avoid tiny candles
    for thr in (0.3, 0.5):
        out[f"candle:range_atr_ge_{thr}"] = (range_atr >= thr, range_atr >= thr)

    # body filter
    for thr in (0.2, 0.4, 0.6):
        out[f"candle:body_atr_ge_{thr}"] = (body_atr >= thr, body_atr >= thr)

    return out


def make_ema200_regime_filter_masks(df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """Macro regime using EMA200 slope (4h) and EMA9-EMA200 distance (extra thresholds)."""
    n = df.shape[0]
    allow_all = np.ones(n, dtype=np.bool_)
    out: Dict[str, Tuple[np.ndarray, np.ndarray]] = {"ind:none": (allow_all, allow_all)}

    slope = df["ema200_slope_4h"].to_numpy()
    dist = df["dist_9_200"].to_numpy()

    out["ema200:slope_pos_long_slope_neg_short"] = (slope > 0.0, slope < 0.0)
    for thr in (0.0005, 0.001, 0.002):  # 0.05%, 0.1%, 0.2% over 4h
        out[f"ema200:abs_slope_ge_{thr}"] = (np.abs(slope) >= thr, np.abs(slope) >= thr)

    # distance regimes (on top of base min distance)
    for thr in (0.006, 0.01, 0.02):
        out[f"ema200:dist_9_200_ge_{thr}"] = (dist >= thr, dist >= thr)

    # sometimes helps to avoid extremely stretched trends
    for thr in (0.03, 0.05):
        out[f"ema200:dist_9_200_le_{thr}"] = (dist <= thr, dist <= thr)

    return out


def build_indicator_group_maps(df: pd.DataFrame) -> Dict[str, Dict[str, Tuple[np.ndarray, np.ndarray]]]:
    """
    Returns:
      { "ind:vol": {filter_name: (long_mask, short_mask), ...}, ... }
    Each group includes an "ind:none" entry for allow-all.
    """
    return {
        "ind:vol": make_volume_filter_masks(df),
        "ind:rsi": make_rsi_filter_masks(df),
        "ind:atr": make_atr_regime_filter_masks(df),
        "ind:adx": make_adx_filter_masks(df),
        "ind:roc": make_roc_filter_masks(df),
        "ind:bb":  make_bb_filter_masks(df),
        "ind:candle": make_candle_size_filter_masks(df),
        "ind:ema200": make_ema200_regime_filter_masks(df),
    }


# ==============================
# SPLITS + CAGR helpers
# ==============================

def compute_train_test_slices(df: pd.DataFrame) -> Tuple[slice, slice]:
    train_end_dt = (
        datetime.strptime(TRAIN_END_DATE, "%Y-%m-%d").replace(tzinfo=LOCAL_TZ)
        + timedelta(days=1) - timedelta(seconds=1)
    )
    train_end_pos = int(df.index.searchsorted(train_end_dt, side="right"))
    train_slice = slice(0, train_end_pos)
    test_slice = slice(train_end_pos, len(df))
    return train_slice, test_slice


def _segment_years() -> Tuple[float, float]:
    """Compute segment lengths in years (calendar-based) from config dates."""
    start = datetime.strptime(START_DATE, "%Y-%m-%d").replace(tzinfo=LOCAL_TZ)
    train_end = datetime.strptime(TRAIN_END_DATE, "%Y-%m-%d").replace(tzinfo=LOCAL_TZ) + timedelta(days=1) - timedelta(seconds=1)
    oos_start = datetime.strptime(TRAIN_END_DATE, "%Y-%m-%d").replace(tzinfo=LOCAL_TZ) + timedelta(days=1)
    end = datetime.strptime(END_DATE, "%Y-%m-%d").replace(tzinfo=LOCAL_TZ) + timedelta(days=1) - timedelta(seconds=1)

    train_years = max(1e-9, (train_end - start).total_seconds() / (365.25 * 24 * 3600))
    oos_years = max(1e-9, (end - oos_start).total_seconds() / (365.25 * 24 * 3600)) if end > oos_start else 1e-9
    return train_years, oos_years


def pct_to_cagr(pct_return: float, years: float) -> float:
    """Convert compounded percent return over 'years' into CAGR percent."""
    if years <= 0:
        return 0.0
    base = 1.0 + (pct_return / 100.0)
    if base <= 0:
        return -100.0
    return (base ** (1.0 / years) - 1.0) * 100.0


# ==============================
# EVALUATION
# ==============================

def evaluate_config(
    df: pd.DataFrame,
    long_base: np.ndarray,
    short_base: np.ndarray,
    cfg: FilterConfig,
    calendar_map: Dict[str, Tuple[np.ndarray, np.ndarray]],
    indicator_maps: Dict[str, Dict[str, Tuple[np.ndarray, np.ndarray]]],
    seg: Optional[slice],
) -> Dict[str, float]:
    if seg is None:
        seg = slice(0, len(df))

    sub_df = df.iloc[seg]
    if len(sub_df) < 5:
        return {
            "total_trades": 0.0,
            "win_rate": 0.0,
            "simple_return_pct": 0.0,
            "compounded_return_pct": 0.0,
            "max_drawdown_pct": 0.0,
            "sharpe_ratio": 0.0,
            "profit_factor": 0.0,
            "avg_trade_return_pct": 0.0,
            "final_equity": INITIAL_CAPITAL,
        }

    sub_long_base = long_base[seg]
    sub_short_base = short_base[seg]

    # calendar masks
    cal_long, cal_short = calendar_map[cfg.calendar_filter]
    cal_long = cal_long[seg]
    cal_short = cal_short[seg]

    # indicator masks (single group)
    if cfg.indicator_group == "ind:none":
        ind_long = np.ones_like(cal_long, dtype=np.bool_)
        ind_short = np.ones_like(cal_short, dtype=np.bool_)
    else:
        group = indicator_maps[cfg.indicator_group]
        ind_long, ind_short = group[cfg.indicator_filter]
        ind_long = ind_long[seg]
        ind_short = ind_short[seg]

    long_mask = cal_long & ind_long
    short_mask = cal_short & ind_short

    if cfg.side_filter == "side:both":
        pass
    elif cfg.side_filter == "side:long_only":
        short_mask = np.zeros_like(short_mask, dtype=np.bool_)
    elif cfg.side_filter == "side:short_only":
        long_mask = np.zeros_like(long_mask, dtype=np.bool_)
    else:
        raise ValueError(f"Unknown side_filter: {cfg.side_filter}")

    long_signal = sub_long_base & long_mask
    short_signal = sub_short_base & short_mask

    o = sub_df["open"].to_numpy(dtype=np.float64)
    h = sub_df["high"].to_numpy(dtype=np.float64)
    l = sub_df["low"].to_numpy(dtype=np.float64)
    c = sub_df["close"].to_numpy(dtype=np.float64)

    (
        total_trades,
        win_rate,
        simple_ret_pct,
        cmpd_ret_pct,
        max_dd_pct,
        sharpe,
        profit_factor,
        avg_return_pct,
        final_equity,
    ) = _simulate_one(
        o, h, l, c,
        long_signal.astype(np.bool_),
        short_signal.astype(np.bool_),
        INITIAL_CAPITAL,
        RISK_PER_TRADE,
        TAKE_PROFIT_R_MULTIPLE,
        BROKERAGE_HALF_RATE,
        SLIPPAGE_HALF_RATE,
    )

    return {
        "total_trades": float(total_trades),
        "win_rate": float(win_rate),
        "simple_return_pct": float(simple_ret_pct),
        "compounded_return_pct": float(cmpd_ret_pct),
        "max_drawdown_pct": float(max_dd_pct),
        "sharpe_ratio": float(sharpe),
        "profit_factor": float(profit_factor),
        "avg_trade_return_pct": float(avg_return_pct),
        "final_equity": float(final_equity),
    }


# ==============================
# DETAILED BACKTEST FOR BEST CONFIG
# ==============================

def backtest_detailed(
    df: pd.DataFrame,
    long_signal: pd.Series,
    short_signal: pd.Series,
    timeframe: str,
) -> List[Dict]:
    trades: List[Dict] = []
    equity = INITIAL_CAPITAL
    current_trade = None

    times = df.index.to_list()

    for i in range(1, len(df)):
        time_i = times[i]
        row = df.iloc[i]
        prev_row = df.iloc[i - 1]

        bar_high = row["high"]
        bar_low = row["low"]

        if current_trade is not None:
            side = current_trade["side"]
            stop_loss = current_trade["stop_loss"]
            target = current_trade["target"]
            entry_price = current_trade["entry_price"]
            risk_per_unit = current_trade["risk_per_unit"]

            # Update MFE/MAE
            if risk_per_unit > 0:
                if side == "long":
                    favourable = (bar_high - entry_price) / risk_per_unit
                    adverse = (entry_price - bar_low) / risk_per_unit
                else:
                    favourable = (entry_price - bar_low) / risk_per_unit
                    adverse = (bar_high - entry_price) / risk_per_unit

                current_trade["mfe_r"] = max(current_trade["mfe_r"], favourable)
                current_trade["mae_r"] = max(current_trade["mae_r"], adverse)

            exit_price_raw = None
            exit_reason = None

            if side == "long":
                stop_hit = bar_low <= stop_loss
                target_hit = bar_high >= target
                if stop_hit and target_hit:
                    exit_price_raw = stop_loss
                    exit_reason = "stop+target_same_bar_stop_first"
                elif stop_hit:
                    exit_price_raw = stop_loss
                    exit_reason = "stop"
                elif target_hit:
                    exit_price_raw = target
                    exit_reason = "target"
            else:
                stop_hit = bar_high >= stop_loss
                target_hit = bar_low <= target
                if stop_hit and target_hit:
                    exit_price_raw = stop_loss
                    exit_reason = "stop+target_same_bar_stop_first"
                elif stop_hit:
                    exit_price_raw = stop_loss
                    exit_reason = "stop"
                elif target_hit:
                    exit_price_raw = target
                    exit_reason = "target"

            if exit_price_raw is not None:
                side = current_trade["side"]
                entry_price = current_trade["entry_price"]
                size = current_trade["size"]
                entry_equity = current_trade["equity_at_entry"]

                # slippage
                if side == "long":
                    entry_price_eff = entry_price * (1.0 + SLIPPAGE_HALF_RATE)
                    exit_price_eff = exit_price_raw * (1.0 - SLIPPAGE_HALF_RATE)
                    gross_pnl = (exit_price_eff - entry_price_eff) * size
                else:
                    entry_price_eff = entry_price * (1.0 - SLIPPAGE_HALF_RATE)
                    exit_price_eff = exit_price_raw * (1.0 + SLIPPAGE_HALF_RATE)
                    gross_pnl = (entry_price_eff - exit_price_eff) * size

                # brokerage
                cost_entry = abs(entry_price_eff * size) * BROKERAGE_HALF_RATE
                cost_exit = abs(exit_price_eff * size) * BROKERAGE_HALF_RATE
                brokerage_cost = cost_entry + cost_exit

                net_pnl = gross_pnl - brokerage_cost
                trade_return = net_pnl / entry_equity if entry_equity != 0 else 0.0
                equity = equity * (1.0 + trade_return)

                r_multiple = trade_return / RISK_PER_TRADE if RISK_PER_TRADE > 0 else 0.0

                trades.append({
                    "symbol": SYMBOL,
                    "timeframe": timeframe,
                    "entry_time": current_trade["entry_time"],
                    "exit_time": time_i,
                    "side": side,
                    "entry_price_raw": entry_price,
                    "exit_price_raw": exit_price_raw,
                    "entry_price_eff": entry_price_eff,
                    "exit_price_eff": exit_price_eff,
                    "stop_loss": current_trade["stop_loss"],
                    "target": current_trade["target"],
                    "size": size,
                    "risk_per_unit": current_trade["risk_per_unit"],
                    "mfe_r": current_trade["mfe_r"],
                    "mae_r": current_trade["mae_r"],
                    "gross_pnl": gross_pnl,
                    "brokerage_cost": brokerage_cost,
                    "pnl": net_pnl,
                    "return_pct": trade_return * 100.0,
                    "r_multiple": r_multiple,
                    "exit_reason": exit_reason,
                    "equity_after": equity,
                    "calendar_filter": current_trade["calendar_filter"],
                    "side_filter": current_trade["side_filter"],
                    "indicator_group": current_trade["indicator_group"],
                    "indicator_filter": current_trade["indicator_filter"],
                })

                current_trade = None
                continue

        # New entries based on previous candle
        if current_trade is None:
            if bool(long_signal.iloc[i - 1]):
                entry_price = prev_row["high"]
                stop_loss = prev_row["low"]
                risk_per_unit = entry_price - stop_loss
                if risk_per_unit > 0 and bar_high >= entry_price:
                    equity_at_entry = equity
                    risk_amount = equity_at_entry * RISK_PER_TRADE
                    size = risk_amount / risk_per_unit
                    target = entry_price + TAKE_PROFIT_R_MULTIPLE * risk_per_unit
                    current_trade = {
                        "side": "long",
                        "entry_time": time_i,
                        "entry_price": entry_price,
                        "stop_loss": stop_loss,
                        "target": target,
                        "size": size,
                        "equity_at_entry": equity_at_entry,
                        "risk_per_unit": risk_per_unit,
                        "mfe_r": 0.0,
                        "mae_r": 0.0,
                        # metadata
                        "calendar_filter": "",
                        "side_filter": "",
                        "indicator_group": "",
                        "indicator_filter": "",
                    }
                    continue

            if bool(short_signal.iloc[i - 1]):
                entry_price = prev_row["low"]
                stop_loss = prev_row["high"]
                risk_per_unit = stop_loss - entry_price
                if risk_per_unit > 0 and bar_low <= entry_price:
                    equity_at_entry = equity
                    risk_amount = equity_at_entry * RISK_PER_TRADE
                    size = risk_amount / risk_per_unit
                    target = entry_price - TAKE_PROFIT_R_MULTIPLE * risk_per_unit
                    current_trade = {
                        "side": "short",
                        "entry_time": time_i,
                        "entry_price": entry_price,
                        "stop_loss": stop_loss,
                        "target": target,
                        "size": size,
                        "equity_at_entry": equity_at_entry,
                        "risk_per_unit": risk_per_unit,
                        "mfe_r": 0.0,
                        "mae_r": 0.0,
                        # metadata
                        "calendar_filter": "",
                        "side_filter": "",
                        "indicator_group": "",
                        "indicator_filter": "",
                    }
                    continue

    # EOD close
    if current_trade is not None:
        last_time = times[-1]
        last_close = df.iloc[-1]["close"]
        side = current_trade["side"]
        entry_price = current_trade["entry_price"]
        size = current_trade["size"]
        entry_equity = current_trade["equity_at_entry"]

        if side == "long":
            entry_price_eff = entry_price * (1.0 + SLIPPAGE_HALF_RATE)
            exit_price_eff = last_close * (1.0 - SLIPPAGE_HALF_RATE)
            gross_pnl = (exit_price_eff - entry_price_eff) * size
        else:
            entry_price_eff = entry_price * (1.0 - SLIPPAGE_HALF_RATE)
            exit_price_eff = last_close * (1.0 + SLIPPAGE_HALF_RATE)
            gross_pnl = (entry_price_eff - exit_price_eff) * size

        cost_entry = abs(entry_price_eff * size) * BROKERAGE_HALF_RATE
        cost_exit = abs(exit_price_eff * size) * BROKERAGE_HALF_RATE
        brokerage_cost = cost_entry + cost_exit
        net_pnl = gross_pnl - brokerage_cost

        trade_return = net_pnl / entry_equity if entry_equity != 0 else 0.0
        equity = equity * (1.0 + trade_return)
        r_multiple = trade_return / RISK_PER_TRADE if RISK_PER_TRADE > 0 else 0.0

        trades.append({
            "symbol": SYMBOL,
            "timeframe": timeframe,
            "entry_time": current_trade["entry_time"],
            "exit_time": last_time,
            "side": side,
            "entry_price_raw": entry_price,
            "exit_price_raw": last_close,
            "entry_price_eff": entry_price_eff,
            "exit_price_eff": exit_price_eff,
            "stop_loss": current_trade["stop_loss"],
            "target": current_trade["target"],
            "size": size,
            "risk_per_unit": current_trade["risk_per_unit"],
            "mfe_r": current_trade["mfe_r"],
            "mae_r": current_trade["mae_r"],
            "gross_pnl": gross_pnl,
            "brokerage_cost": brokerage_cost,
            "pnl": net_pnl,
            "return_pct": trade_return * 100.0,
            "r_multiple": r_multiple,
            "exit_reason": "eod",
            "equity_after": equity,
            "calendar_filter": current_trade["calendar_filter"],
            "side_filter": current_trade["side_filter"],
            "indicator_group": current_trade["indicator_group"],
            "indicator_filter": current_trade["indicator_filter"],
        })

    return trades


# ==============================
# SEARCH LOGIC (50k+, low overfit)
# ==============================

def _make_result_row(
    timeframe: str,
    cfg: FilterConfig,
    full_m: Dict[str, float],
    train_m: Dict[str, float],
    oos_m: Dict[str, float],
    train_years: float,
    oos_years: float,
) -> Dict:
    train_cagr = pct_to_cagr(train_m["compounded_return_pct"], train_years)
    oos_cagr = pct_to_cagr(oos_m["compounded_return_pct"], oos_years)
    robust_cagr = min(train_cagr, oos_cagr)
    avg_cagr = 0.5 * (train_cagr + oos_cagr)

    return {
        "symbol": SYMBOL,
        "timeframe": timeframe,
        "config_id": cfg.id(),
        "calendar_filter": cfg.calendar_filter,
        "side_filter": cfg.side_filter,
        "indicator_group": cfg.indicator_group,
        "indicator_filter": cfg.indicator_filter,

        "full_trades": full_m["total_trades"],
        "full_win_rate": full_m["win_rate"],
        "full_cmpd_ret_pct": full_m["compounded_return_pct"],
        "full_max_dd_pct": full_m["max_drawdown_pct"],

        "train_trades": train_m["total_trades"],
        "train_win_rate": train_m["win_rate"],
        "train_cmpd_ret_pct": train_m["compounded_return_pct"],
        "train_cagr_pct": train_cagr,

        "oos_trades": oos_m["total_trades"],
        "oos_win_rate": oos_m["win_rate"],
        "oos_cmpd_ret_pct": oos_m["compounded_return_pct"],
        "oos_cagr_pct": oos_cagr,

        "robust_cagr_pct": robust_cagr,
        "avg_cagr_pct": avg_cagr,
    }


def run_search_for_timeframe(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
    calendar_map = build_calendar_map(df)
    indicator_maps = build_indicator_group_maps(df)

    long_base = df["long_signal_raw"].to_numpy(dtype=np.bool_)
    short_base = df["short_signal_raw"].to_numpy(dtype=np.bool_)

    train_slice, oos_slice = compute_train_test_slices(df)
    train_years, oos_years = _segment_years()

    side_options = ["side:both", "side:long_only", "side:short_only"]

    # ---- Systematic sweeps (guarantee coverage of "only time", "only RSI", etc.)
    results: List[Dict] = []
    seen = set()

    def eval_and_add(cfg: FilterConfig) -> None:
        cid = cfg.id()
        if cid in seen:
            return
        seen.add(cid)

        full_m = evaluate_config(df, long_base, short_base, cfg, calendar_map, indicator_maps, None)
        train_m = evaluate_config(df, long_base, short_base, cfg, calendar_map, indicator_maps, train_slice)
        oos_m = evaluate_config(df, long_base, short_base, cfg, calendar_map, indicator_maps, oos_slice)

        results.append(_make_result_row(timeframe, cfg, full_m, train_m, oos_m, train_years, oos_years))

    # Baseline
    eval_and_add(FilterConfig("cal:none", "side:both", "ind:none", "ind:none"))
    # Side-only baselines
    for side in ("side:long_only", "side:short_only"):
        eval_and_add(FilterConfig("cal:none", side, "ind:none", "ind:none"))

    print("\nPhase A1: Calendar-only sweeps (time-of-day + DOW)...")
    calendar_keys = list(calendar_map.keys())
    for cal in calendar_keys:
        for side in side_options:
            eval_and_add(FilterConfig(cal, side, "ind:none", "ind:none"))

    print("\nPhase A2: Indicator-only sweeps (one indicator group at a time)...")
    for group_name, group_map in indicator_maps.items():
        # skip the shared "ind:none" key inside each map; configs use group_name and filter
        keys = [k for k in group_map.keys() if k != "ind:none"]
        for k in keys:
            for side in side_options:
                eval_and_add(FilterConfig("cal:none", side, group_name, k))

    # ---- Random search: calendar + single indicator group (or calendar-only / indicator-only)
    print(f"\nPhase B: Random low-overfit configs (target {N_RANDOM_CONFIGS} unique)...")
    rng = np.random.default_rng(RANDOM_SEED)

    indicator_groups = list(indicator_maps.keys())

    # pre-list indicator keys per group (excluding ind:none)
    ind_keys_by_group: Dict[str, List[str]] = {}
    for g in indicator_groups:
        ind_keys_by_group[g] = [k for k in indicator_maps[g].keys() if k != "ind:none"]

    def random_calendar() -> str:
        # 70% use time/DOW filter, 30% none
        if rng.random() < 0.30:
            return "cal:none"
        return calendar_keys[int(rng.integers(0, len(calendar_keys)))]

    def random_indicator() -> Tuple[str, str]:
        # 35% none, 65% pick one indicator group
        if rng.random() < 0.35:
            return ("ind:none", "ind:none")
        g = indicator_groups[int(rng.integers(0, len(indicator_groups)))]
        keys = ind_keys_by_group[g]
        k = keys[int(rng.integers(0, len(keys)))]
        return (g, k)

    added = 0
    attempts = 0
    max_attempts = N_RANDOM_CONFIGS * 200  # avoid infinite loops

    while added < N_RANDOM_CONFIGS and attempts < max_attempts:
        attempts += 1
        cal = random_calendar()
        side = side_options[int(rng.integers(0, len(side_options)))]
        g, k = random_indicator()

        cfg = FilterConfig(cal, side, g, k)
        cid = cfg.id()
        if cid in seen:
            continue

        eval_and_add(cfg)
        added += 1
        if added % 1000 == 0:
            print(f"  random configs evaluated: {added}/{N_RANDOM_CONFIGS}")

    out_df = pd.DataFrame(results)

    # Validity flags
    out_df["valid_full"] = out_df["full_trades"] >= MIN_TRADES_FULL_FOR_RANK
    out_df["valid_train"] = out_df["train_trades"] >= MIN_TRADES_TRAIN_FOR_RANK
    out_df["valid_oos"] = out_df["oos_trades"] >= MIN_TRADES_OOS_FOR_RANK

    if REQUIRE_POSITIVE_TRAIN_AND_OOS:
        out_df["valid_return"] = (out_df["train_cmpd_ret_pct"] > 0) & (out_df["oos_cmpd_ret_pct"] > 0)
    else:
        out_df["valid_return"] = True

    # Ranking: prioritize robust CAGR, then average CAGR, then OOS CAGR, then trade counts
    out_df = out_df.sort_values(
        ["valid_oos", "valid_train", "valid_full", "valid_return", "robust_cagr_pct", "avg_cagr_pct", "oos_cagr_pct", "oos_trades", "train_trades"],
        ascending=[False, False, False, False, False, False, False, False, False],
    ).reset_index(drop=True)

    print("\nTop 10 configs (ranked by robust_cagr_pct = min(train_cagr, oos_cagr)):")
    show_cols = [
        "robust_cagr_pct", "avg_cagr_pct",
        "train_cagr_pct", "oos_cagr_pct",
        "train_cmpd_ret_pct", "oos_cmpd_ret_pct",
        "train_trades", "oos_trades",
        "calendar_filter", "side_filter", "indicator_group", "indicator_filter",
    ]
    print(out_df.head(10)[show_cols].to_string(index=False))

    return out_df


# ==============================
# MAIN
# ==============================

def main() -> None:
    if njit is None:
        raise RuntimeError("Numba is required. Install: pip install numba")

    all_results: List[pd.DataFrame] = []

    for res in TIMEFRAMES:
        print("\n==============================================")
        print(f"Running timeframe: {res}")
        print("==============================================")

        df = fetch_ohlc_from_delta(SYMBOL, res, START_DATE, END_DATE)
        df = add_indicators(df, resolution=res)
        df = build_base_signals(df)

        results_df = run_search_for_timeframe(df, res)
        all_results.append(results_df)

    final_df = pd.concat(all_results, ignore_index=True)
    # IMPORTANT: if you test multiple timeframes, concat() does not guarantee global ranking.
    # Re-sort globally using the same ranking keys used per-timeframe.
    final_df = final_df.sort_values(
        ['valid_oos', 'valid_train', 'valid_full', 'valid_return',
         'robust_cagr_pct', 'avg_cagr_pct', 'oos_cagr_pct', 'oos_trades', 'train_trades'],
        ascending=[False, False, False, False, False, False, False, False, False],
    ).reset_index(drop=True)
    final_df.to_csv(RESULTS_SUMMARY_CSV_PATH, index=False)
    print(f"\nSaved summary results to: {RESULTS_SUMMARY_CSV_PATH} ({len(final_df)} rows)")

    # Save a separate CSV with the TOP-N configs for quick review
    top_n = int(min(TOP_N_FILTERS_TO_SAVE, len(final_df)))
    top_cols = [
        'symbol', 'timeframe', 'config_id',
        'robust_cagr_pct', 'avg_cagr_pct',
        'train_cagr_pct', 'oos_cagr_pct',
        'train_cmpd_ret_pct', 'oos_cmpd_ret_pct', 'full_cmpd_ret_pct',
        'train_trades', 'oos_trades', 'full_trades',
        'train_win_rate', 'oos_win_rate', 'full_win_rate',
        'full_max_dd_pct',
        'calendar_filter', 'side_filter', 'indicator_group', 'indicator_filter',
        'valid_train', 'valid_oos', 'valid_full', 'valid_return',
    ]
    final_df.head(top_n)[top_cols].to_csv(TOP_FILTERS_CSV_PATH, index=False)
    print(f"Saved top {TOP_N_FILTERS_TO_SAVE} configs to: {TOP_FILTERS_CSV_PATH} (rows={top_n})")

    best_row = final_df.iloc[0]
    print("\nBest config selected (robust on TRAIN + OOS):")
    print(best_row[[
        "robust_cagr_pct", "avg_cagr_pct",
        "train_cagr_pct", "oos_cagr_pct",
        "train_cmpd_ret_pct", "oos_cmpd_ret_pct",
        "train_trades", "oos_trades",
        "calendar_filter", "side_filter", "indicator_group", "indicator_filter",
    ]].to_string())

    # Build trade log for best config
    res0 = TIMEFRAMES[0]
    df0 = fetch_ohlc_from_delta(SYMBOL, res0, START_DATE, END_DATE)
    df0 = add_indicators(df0, resolution=res0)
    df0 = build_base_signals(df0)

    calendar_map = build_calendar_map(df0)
    indicator_maps = build_indicator_group_maps(df0)

    long_base = df0["long_signal_raw"].to_numpy(dtype=np.bool_)
    short_base = df0["short_signal_raw"].to_numpy(dtype=np.bool_)

    best_cfg = FilterConfig(
        calendar_filter=str(best_row["calendar_filter"]),
        side_filter=str(best_row["side_filter"]),
        indicator_group=str(best_row["indicator_group"]),
        indicator_filter=str(best_row["indicator_filter"]),
    )

    cal_long, cal_short = calendar_map[best_cfg.calendar_filter]
    if best_cfg.indicator_group == "ind:none":
        ind_long = np.ones_like(cal_long, dtype=np.bool_)
        ind_short = np.ones_like(cal_short, dtype=np.bool_)
    else:
        ind_long, ind_short = indicator_maps[best_cfg.indicator_group][best_cfg.indicator_filter]

    long_mask = cal_long & ind_long
    short_mask = cal_short & ind_short

    if best_cfg.side_filter == "side:long_only":
        short_mask = np.zeros_like(short_mask, dtype=np.bool_)
    elif best_cfg.side_filter == "side:short_only":
        long_mask = np.zeros_like(long_mask, dtype=np.bool_)

    long_sig = pd.Series(long_base & long_mask, index=df0.index)
    short_sig = pd.Series(short_base & short_mask, index=df0.index)

    trades = backtest_detailed(df0, long_sig, short_sig, timeframe=res0)

    # attach config metadata to each trade record (so you know which filter produced it)
    if trades:
        for t in trades:
            t["calendar_filter"] = best_cfg.calendar_filter
            t["side_filter"] = best_cfg.side_filter
            t["indicator_group"] = best_cfg.indicator_group
            t["indicator_filter"] = best_cfg.indicator_filter

        trades_df = pd.DataFrame(trades)
        trades_df.to_csv(BEST_TRADES_CSV_PATH, index=False)
        print(f"\nSaved best config trades to: {BEST_TRADES_CSV_PATH} ({len(trades_df)} trades)")
    else:
        print("\nBest config produced no trades; trades CSV not saved.")


if __name__ == "__main__":
    main()